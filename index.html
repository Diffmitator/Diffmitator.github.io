<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Hugo 0.66.0" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,600" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
  <link rel="stylesheet" href="../css/normalize.css">
  <link rel="stylesheet" href="../css/skeleton.css">
  <link rel="stylesheet" href="../css/custom.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="alternate" href="index.xml" type="application/rss+xml" title="Speech Research">
  <link rel="shortcut icon" href="favicon.png" type="image/x-icon" />
  <title>Diffmitator: Classification Feature-Guided Audio Generation with Training-free Timbre Control</title>
</head>

<body rightmargin="150" leftmargin="150" topmargin="100" bottommargin="100" line-height:160%>
  <font size="5">

    <div class="container">

      <header role="banner">

      </header>
      <main role="main">
        <article itemscope itemtype="https://schema.org/BlogPosting">
          <br></br>
          <div style="display: flex; align-items: center; justify-content: center;">
            <!-- 图片 -->
            <!-- <img src="baton_cartoon_sam.png" alt="Conductor's Baton" style="width: auto; height: 100px;">  -->
            <!-- 标题的第一部分 -->
            <h1 itemprop="headline" style="margin-left: 20px;">Diffmitator: Classification Feature-Guided Audio Generation with Training-free Timbre Control</h1>
          </div>         
          
          <br>
          <p style="line-height:1" align="center"><b>
              <font color="061E61">Anonymous</font>
            </b></p>
          
          <section itemprop="entry-text">
            <br>
            <h2 id="abstract">
              <font color="000093">Abstract</font>
            </h2>
            <p style="text-align: justify;">
              <font color="061E61"> The demand for deep learning-based audio generation has grown significantly in various multimedia and gaming domains, but its application to sound effect generation remains largely limited due to several challenges. Although previous work has used diffusion models to address the cross-modal sound effect generation task, two issues remain: the trade-off between diffusion model performance and training cost, and the timbre inconsistency between reference and generated audio. To address these challenges, we propose two methods: D-HTSAT and C-Copaint. To reduce the difficulty of model training, we present D-HTSAT, an audio classification feature-based model that can decompose the audio generation task into semantic content generation and timbre content generation sub-tasks. C-Copaint is a training-free method that addresses the issue of inconsistent timbre generation by transforming the problem into a conditional inpainting task, which ensures consistent timbre through monitoring during the DDIM denoising process. Based on D-HTSAT and C-Copaint, we build Diffmitator -- a model that combines language model and diffusion model for conditional sound effect generation. Objective and subjective experiments demonstrate that Diffmitator effectively improves the quality of generated sound effects compared to existing methods. Demos are available at: https://diffmitator.github.io/
              </font>
            </p>

            <br>
            <figure>
            <h2 id="pipeline">
              <font color="000093">Pipeline</font>
            </h2>
              <p align="center"><img src="paper2_preview.png" width="100%" class="center" /></p>
              <figcaption>
                <p style="text-align: justify;">
                  <font color="061E61"><b>Figure 1:</b> Diffmitator is composed of GPT-2, LDM, VAE on the Mel-spectrogram and a vocoder to recover audio signals in time domain. GPT-2 generates D-HTSAT features based on the input conditions, thereby completing audio semantic content generation. The VAE maps the audio Mel-spectrogram to a smaller latent feature space. LDM performs denoising operations in this latent space, obtaining the latent feature representation of the target audio based on the D-HTSAT features generated by GPT-2.
                </p>
              </figcaption>
            </figure>

            <br></br>
            <h2 id="Text-to-Audio Generated Results">
              <font color="000093">Text-to-Audio Generated Results</font>
            </h2>

            <br></br>
            <h2 id="Video-to-Audio Generated Results">
              <font color="000093">Video-to-Audio Generated Results</font>
            </h2>

            <br></br>
            <h2 id="C-Copaint Generated Results">
              <font color="000093">C-Copaint Generated Results</font>
            </h2>


            <h2 id="Acknowledgement">
              <font color="000093">Acknowledgement</font>
            </h2>
            <div class="container">
              <p>
                Thanks for the excellent open-source project demo page template provided by <a href="https://github.com/AudioLDM/AudioLDM.github.io">AudioLDM</a>.
              </p>
              
            </div>
          </section>
        </article>
      </main>

    </div>

    <script>
      (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
          m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
      ga('create', 'UA-139981676-1', 'auto');
      ga('send', 'pageview');
    </script>

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>



    <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

    <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
      </script>




</body>

</html>
